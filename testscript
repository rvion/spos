#!/bin/bash
hosts='olivier-All-Series olivier-Bell olivier-Aspire-M7811'
echo $hosts
for host in $hosts
do 
	echo trying $host 
	#	
	if eval 'docker  -H tcp://$host:2375 info'
	then
		echo doker daemon already running on tcp://$host:2375
	else
		echo trying to start docker daemon on $host		
		#add 	"sudo service docker.io restart;" ?	
		if $host==$(hostname)		
		then
			gnome-terminal -x 'sudo docker daemon -H tcp://0.0.0.0:2375;/bin/bash'		
		else
			eval gnome-terminal -x 'ssh $USER@$host "sudo docker daemon -H tcp://0.0.0.0:2375;/bin/bash"'
		fi
	fi
done 

SWARM_ID=$(docker run --rm swarm create)


echo new swarm created : $SWARM_ID 



for host in $hosts
do 
	echo '##################################################'	
	echo cleaning $host	
	echo '#'	
	ssh $USER@$host 'docker -H tcp://0.0.0.0:2375 rm -f $(docker -H tcp://0.0.0.0:2375 ps -q -a)'	
	
	ssh $USER@$host \
        "docker -H tcp://0.0.0.0:2375 run -d swarm join --addr=\$(hostname -I | awk '{print \$1}'):2375 token://$SWARM_ID"
done 

docker -H tcp://0.0.0.0:2375 run -d -p 9999:2375 swarm manage token://$SWARM_ID

echo 'sleeping for 10 s'
sleep 10

alias dswarm='docker -H tcp://0.0.0.0:9999'

for cont in $(dswarm ps -q)
do
  dswarm rm -f $cont
done
############################################################
echo 'launching master'
dswarm run\
 --net=host -d \
 --name sparkmaster\
 randompulse/newspark:latest\
 bash -c "./bin/spark-class org.apache.spark.deploy.master.Master"
# bash -c "SPARK_MASTER_IP=\$(hostname -I | awk '{print \$1}') ./bin/spark-class org.apache.spark.deploy.master.Master"

############################################################
#or to debug
#  dswarm run\
#   --net=host --rm -it\
#   --name sparkmaster\
#    randompulse/newspark:latest \
#    bash -c "SPARK_MASTER_IP=\$(hostname -I | awk '{print \$1}') ./bin/spark-class org.apache.spark.deploy.master.Master"

############################################################

sleep 5

MASTER_IP=""
MASTER_IP=$(dswarm inspect --format '{{ .Node.IP }}' sparkmaster)
if [[ -z $MASTER_IP ]]
then
	echo failed to create master
	return
else
	echo '############################################################'
	echo '#'
	echo look for master UI at $MASTER_IP:8082
fi

############################################################

echo '#'
echo '#'
echo '############################################################'
echo 'launching workers'
for x in 1 2 3
do
dswarm run\
 -d\
 --name sparkworker$x\
 --net=host \
 randompulse/newspark:latest \
 bash -c "./bin/spark-class org.apache.spark.deploy.worker.Worker spark://$MASTER_IP:7077"
# bash -c "SPARK_LOCAL_IP=\$(hostname -I | awk '{print \$1}') ./bin/spark-class org.apache.spark.deploy.worker.Worker spark://$MASTER_IP:7077"
done
# same with sparkworker2 etc...
#run --rm -it to debug
echo '############################################################'

echo '######################################################################'

echo '######################################################################'
echo '############################################################'
dswarm run\
 --rm -it\
 --net=host\
 --name sparkshell\
 randompulse/newspark:latest \
 bash -c "./bin/spark-shell --master spark://$MASTER_IP:7077"
# bash -c "SPARK_LOCAL_IP=\$(hostname -I | awk '{print \$1}') ./bin/spark-shell --master spark://$MASTER_IP:7077"


#then send to scala>
#val NUM_SAMPLES=Math.pow(10,8).toInt
#    val count = sc.parallelize(1 until NUM_SAMPLES,40).map { i => val 
#       x = Math.random() * 2 - 1
#       val y = Math.random() * 2 - 1
#       if (x*x + y*y < 1) 1 else 0
#    }.reduce(_ + _)
#    println("Pi is roughly " + 4.0 * count / NUM_SAMPLES)
#
#
#
#


#should provide:
#
#>alias dswarm='docker -H tcp://0.0.0.0:9999'
#
#>dswarm info
#Containers: 9
#Images: 34
#Role: primary
#Strategy: spread
#Filters: affinity, health, constraint, port, dependency
#Nodes: 3
# olivier-All-Series: 192.168.1.54:2375
#  └ Containers: 3
#  └ Reserved CPUs: 0 / 8
#  └ Reserved Memory: 0 B / 32.93 GiB
#  └ Labels: executiondriver=native-0.2, kernelversion=3.16.0-49-generic, operatingsystem=Ubuntu 14.04.2 LTS, storagedriver=aufs
# olivier-Aspire-M7811: 192.168.1.28:2375
#  └ Containers: 3
#  └ Reserved CPUs: 0 / 4
#  └ Reserved Memory: 0 B / 8.113 GiB
#  └ Labels: executiondriver=native-0.2, kernelversion=3.16.0-38-generic, operatingsystem=Ubuntu 14.04.2 LTS, storagedriver=aufs
# olivier-Bell: 192.168.1.31:2375
#  └ Containers: 3
#  └ Reserved CPUs: 0 / 4
#  └ Reserved Memory: 0 B / 8.068 GiB
#  └ Labels: executiondriver=native-0.2, kernelversion=3.16.0-30-generic, operatingsystem=Ubuntu 14.04.2 LTS, storagedriver=aufs
#CPUs: 16
#Total Memory: 49.11 GiB
#Name: b9e3aa8c1c08
#
#>dswarm ps
#olivier@olivier-All-Series:~/Documents/tests/test_rancher_spark/spark_on_swarm$ dswarm psCONTAINER ID        IMAGE                         COMMAND                  CREATED             #STATUS              PORTS               NAMES
#86aaae72adbb        randompulse/newspark:latest   "bash -c './bin/spark"   4 minutes ago       Up 4 minutes                            olivier-Bell/sparkshell
#a71abbcd93de        randompulse/newspark:latest   "bash -c './bin/spark"   4 minutes ago       Up 4 minutes                            olivier-All-Series/sparkworker2
#ba404256c879        randompulse/newspark:latest   "bash -c './bin/spark"   4 minutes ago       Up 4 minutes                            olivier-Aspire-M7811/sparkworker3
#21e1c4c53ecf        randompulse/newspark:latest   "bash -c './bin/spark"   4 minutes ago       Up 4 minutes                            olivier-Bell/sparkworker1
#8e05ddc61449        randompulse/newspark:latest   "bash -c './bin/spark"   4 minutes ago       Up 4 minutes                            olivier-Aspire-M7811/sparkmaster


#look for master webui on port 8082
gnome-open http://$(dswarm inspect --format '{{ .Node.IP }}' sparkmaster):8082


